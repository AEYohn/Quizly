#!/usr/bin/env python3
"""
Misconception Auto-Tagger
==========================
Automatically categorizes why students got wrong answers.
Uses LLM to analyze reasoning and identify specific misconceptions.
"""

import os
import json
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


class MisconceptionCategory(Enum):
    """High-level misconception categories."""
    CONCEPTUAL = "conceptual"  # Wrong understanding of concept
    PROCEDURAL = "procedural"  # Wrong method/steps
    CARELESS = "careless"  # Misread or calculation error
    INCOMPLETE = "incomplete"  # Partial understanding
    OVERCONFIDENT = "overconfident"  # Guessed confidently but wrong
    UNKNOWN = "unknown"


class MisconceptionSeverity(Enum):
    """How serious is this misconception."""
    MINOR = "minor"  # Easy to correct
    MODERATE = "moderate"  # Needs some review
    SEVERE = "severe"  # Fundamental misunderstanding


@dataclass
class MisconceptionResult:
    """Result of misconception tagging."""
    student_id: int
    question_id: str
    
    # Classification
    misconception_type: str  # Specific type (e.g., "quantifier_flip")
    category: MisconceptionCategory
    severity: MisconceptionSeverity
    
    # Details
    description: str
    root_cause: str
    evidence: List[str]  # Quotes from reasoning that show the misconception
    
    # Remediation
    suggested_remediation: str
    related_concepts: List[str]
    
    # Confidence in classification
    confidence: float  # 0-1
    
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self) -> Dict:
        return {
            "student_id": self.student_id,
            "question_id": self.question_id,
            "misconception_type": self.misconception_type,
            "category": self.category.value,
            "severity": self.severity.value,
            "description": self.description,
            "root_cause": self.root_cause,
            "evidence": self.evidence,
            "suggested_remediation": self.suggested_remediation,
            "related_concepts": self.related_concepts,
            "confidence": self.confidence,
            "timestamp": self.timestamp
        }


# NOTE: Misconception taxonomy is now dynamically generated by the LLM
# based on the question's concept and domain. No static taxonomy needed.



class MisconceptionTagger:
    """
    LLM-powered misconception tagger.
    
    Analyzes student responses to identify and categorize misconceptions.
    """
    
    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")
        self.model = None
        
        if GEMINI_AVAILABLE and self.api_key:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel("gemini-2.0-flash")
        
        self.misconception_history: List[MisconceptionResult] = []
    
    def tag_response(
        self,
        student_id: int,
        question: Dict[str, Any],
        student_answer: str,
        student_reasoning: str,
        correct_answer: str,
        correct_explanation: Optional[str] = None
    ) -> MisconceptionResult:
        """
        Analyze a wrong response and identify the misconception.
        
        Args:
            student_id: ID of the student
            question: Question dict with prompt, options, concept
            student_answer: The student's (wrong) answer
            student_reasoning: The student's explanation
            correct_answer: The correct answer
            correct_explanation: Optional correct explanation
            
        Returns:
            MisconceptionResult with categorization and remediation
        """
        question_id = question.get("id", str(hash(question.get("prompt", ""))))
        concept = question.get("concept", "unknown")
        
        if not self.model:
            return self._fallback_tagging(
                student_id, question_id, concept,
                student_answer, student_reasoning, correct_answer
            )
        
        prompt = f"""You are an expert at identifying student misconceptions across any academic subject.

## Question
{question.get('prompt', 'N/A')}

## Concept/Topic
{concept}

## Options
{chr(10).join(question.get('options', []))}

## Student's Wrong Answer
{student_answer}

## Student's Reasoning
{student_reasoning}

## Correct Answer
{correct_answer}

## Correct Explanation
{correct_explanation or 'Not provided'}

---

Analyze this wrong answer. Identify the specific misconception the student has about this topic.

Return JSON:
{{
    "misconception_type": "A specific, descriptive name for this misconception (e.g., 'negation_scope_error', 'definition_confusion', 'algorithm_complexity_error')",
    "category": "conceptual|procedural|careless|incomplete|overconfident|unknown",
    "severity": "minor|moderate|severe",
    "description": "One sentence description of the misconception",
    "root_cause": "Why the student likely made this error",
    "evidence": ["quote from reasoning showing misconception"],
    "suggested_remediation": "How to help this student understand",
    "related_concepts": ["concept1", "concept2"],
    "confidence": 0.0-1.0
}}"""

        try:
            response = self.model.generate_content(prompt)
            text = response.text.strip()
            
            start = text.find("{")
            end = text.rfind("}") + 1
            if start >= 0 and end > start:
                result = json.loads(text[start:end])
                
                category = MisconceptionCategory.UNKNOWN
                try:
                    category = MisconceptionCategory(result.get("category", "unknown"))
                except ValueError:
                    pass
                
                severity = MisconceptionSeverity.MODERATE
                try:
                    severity = MisconceptionSeverity(result.get("severity", "moderate"))
                except ValueError:
                    pass
                
                misconception = MisconceptionResult(
                    student_id=student_id,
                    question_id=question_id,
                    misconception_type=result.get("misconception_type", "unknown"),
                    category=category,
                    severity=severity,
                    description=result.get("description", ""),
                    root_cause=result.get("root_cause", ""),
                    evidence=result.get("evidence", []),
                    suggested_remediation=result.get("suggested_remediation", ""),
                    related_concepts=result.get("related_concepts", []),
                    confidence=result.get("confidence", 0.5)
                )
                
                self.misconception_history.append(misconception)
                return misconception
                
        except Exception as e:
            print(f"Misconception tagger error: {e}")
        
        return self._fallback_tagging(
            student_id, question_id, concept,
            student_answer, student_reasoning, correct_answer
        )
    
    def _fallback_tagging(
        self,
        student_id: int,
        question_id: str,
        concept: str,
        student_answer: str,
        student_reasoning: str,
        correct_answer: str
    ) -> MisconceptionResult:
        """Fallback when LLM is unavailable - returns minimal result."""
        # Return minimal result without fake analysis
        # The LLM is required for proper misconception identification
        return MisconceptionResult(
            student_id=student_id,
            question_id=question_id,
            misconception_type="llm_unavailable",
            category=MisconceptionCategory.UNKNOWN,
            severity=MisconceptionSeverity.MODERATE,
            description=f"LLM required for misconception analysis. Student answered {student_answer}, correct was {correct_answer}.",
            root_cause="LLM analysis unavailable",
            evidence=[],
            suggested_remediation="",
            related_concepts=[concept] if concept else [],
            confidence=0.0
        )
    
    def batch_tag(
        self,
        responses: List[Dict[str, Any]]
    ) -> List[MisconceptionResult]:
        """
        Tag multiple wrong responses.
        
        Args:
            responses: List of dicts with student_id, question, answer, reasoning, correct
            
        Returns:
            List of MisconceptionResults
        """
        results = []
        for r in responses:
            if not r.get("is_correct", True):  # Only tag wrong answers
                result = self.tag_response(
                    student_id=r.get("student_id", 0),
                    question=r.get("question", {}),
                    student_answer=r.get("answer", ""),
                    student_reasoning=r.get("reasoning", ""),
                    correct_answer=r.get("correct_answer", "")
                )
                results.append(result)
        return results
    
    def get_misconception_summary(
        self,
        results: Optional[List[MisconceptionResult]] = None
    ) -> Dict[str, Any]:
        """
        Get summary of misconceptions across responses.
        
        Returns:
            Dict with type counts, category distribution, top issues
        """
        if results is None:
            results = self.misconception_history
        
        type_counts: Dict[str, int] = {}
        category_counts: Dict[str, int] = {}
        severity_counts: Dict[str, int] = {}
        
        for r in results:
            # Type counts
            type_counts[r.misconception_type] = type_counts.get(r.misconception_type, 0) + 1
            
            # Category counts
            cat = r.category.value
            category_counts[cat] = category_counts.get(cat, 0) + 1
            
            # Severity counts
            sev = r.severity.value
            severity_counts[sev] = severity_counts.get(sev, 0) + 1
        
        # Sort by frequency
        top_types = sorted(type_counts.items(), key=lambda x: -x[1])[:10]
        
        return {
            "total_misconceptions": len(results),
            "top_misconception_types": top_types,
            "category_distribution": category_counts,
            "severity_distribution": severity_counts,
            "unique_students": len(set(r.student_id for r in results)),
            "remediation_suggestions": list(set(r.suggested_remediation for r in results))[:5]
        }


# Singleton instance
misconception_tagger = MisconceptionTagger()
